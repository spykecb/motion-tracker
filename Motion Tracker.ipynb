{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e52022-9223-4044-be1f-1925566aa09d",
   "metadata": {},
   "source": [
    "# Motion Tracker\n",
    "\n",
    "Let's see how this goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e070c032-84bf-462c-bfb7-dcdd53f9536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[271.3513 300.9344 275.8941 ...   0.     437.951  512.    ]\n",
      " [362.5138 315.8901 361.0373 ...   0.     512.     512.    ]\n",
      " [280.8587 313.8994 280.6007 ...   0.     512.     512.    ]\n",
      " ...\n",
      " [258.1431 179.7763 257.596  ... 140.9268 411.5411 509.9083]\n",
      " [257.6646 167.4352 255.3693 ... 158.8761 424.7657 512.    ]\n",
      " [254.0496 181.4941 254.3449 ... 145.4788 396.5332 498.7738]]\n",
      "0.0 512.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from model import MotionDataset, PositionFinder, BoundingBoxFinder\n",
    "import helper\n",
    "\n",
    "# minmax, minmax_z = helper.get_minmax('train/input.csv', 'test/input.csv')\n",
    "# print(minmax[0], minmax[1])\n",
    "# print(minmax_z[0], minmax_z[1])\n",
    "\n",
    "minmax, minmax_z = helper.get_minmax('train/input.csv', 'test/input.csv')\n",
    "print(minmax[0], minmax[1])\n",
    "# print(minmax_z[0], minmax_z[1])\n",
    "\n",
    "img_width = 256\n",
    "#randomly rotate or transform the images to help training\n",
    "train_transforms = transforms.Compose([\n",
    "#                                         transforms.RandomRotation(30),\n",
    "#                                         transforms.RandomResizedCrop(256),\n",
    "                                        transforms.Resize((img_width,img_width)),\n",
    "                                       transforms.ToTensor()\n",
    "#                                            ,transforms.Normalize([0.5, 0.5, 0.5], \n",
    "#                                                              [0.5, 0.5, 0.5])\n",
    "                                       ,transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                             [0.229, 0.224, 0.225])\n",
    "                                      ])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "                                       transforms.Resize((img_width,img_width)),\n",
    "                                       transforms.ToTensor()\n",
    "#                                         ,transforms.Normalize([0.5, 0.5, 0.5], \n",
    "#                                                              [0.5, 0.5, 0.5])\n",
    "                                           ,transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                                                             [0.229, 0.224, 0.225])\n",
    "                                      ])\n",
    "\n",
    "motions = MotionDataset('train/input.csv', 'train', train_transforms, minmax, minmax_z)\n",
    "trainloader = DataLoader(motions, batch_size=16, shuffle=True)\n",
    "img, details, transform, confidences, boundaries = next(iter(trainloader))\n",
    "\n",
    "motions_test = MotionDataset('test/input.csv', 'test', test_transforms, minmax, minmax_z)\n",
    "testloader = torch.utils.data.DataLoader(motions_test, batch_size=16, shuffle=True)\n",
    "details.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8899ec-6a9c-4ade-993e-f7b0fc2958cb",
   "metadata": {},
   "source": [
    "## Prepare neural network model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9565a269-deca-45fc-8c7a-13f57d46e172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionFinder(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (hidden1): Linear(in_features=131074, out_features=1024, bias=True)\n",
      "  (output): Linear(in_features=1024, out_features=44, bias=True)\n",
      "  (dropout): Dropout(p=0.4, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bmodel = BoundingBoxFinder(img_width)\n",
    "model = PositionFinder(img_width)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b89c61e-f88f-4003-81ac-ddcfa712c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(output, target):\n",
    "    #mseloss (x - y) ^ 2\n",
    "    #l1loss abs(x - y) \n",
    "#     output_2d = np.reshape(output, (-1, 2))\n",
    "#     target_2d = np.reshape(target, (-1, 2))\n",
    "\n",
    "\n",
    "    loss = torch.dist(output,target) # <---- result seemed quite good\n",
    "#     x = np.reshape(output.detach().cpu(), (-1, 2))\n",
    "#     y = np.reshape(target.detach().cpu(), (-1, 2))\n",
    "#     loss = torch.diag(torch.cdist(x,y)).mean()\n",
    "#     loss = 0\n",
    "#     for i in range(output.shape[0]):\n",
    "#         for j in range(0, output.shape[1], 2):\n",
    "#             loss += (torch.norm(output[i][j:j+2] - target[i][j:j+2])) ** 0.5\n",
    "#     loss = loss / output.shape[1]\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be02883b-a2a5-4b4e-af5e-7246d155fdc2",
   "metadata": {},
   "source": [
    "## Train our model\n",
    "\n",
    "best loss (L1Loss Sum, 0.003, conv4)\n",
    "0.001 - 0.003\n",
    "\n",
    "\n",
    "```\n",
    " Epoch 22/25) Training loss: 6.749771996027863, Test loss: 10.508017539978027\n",
    "(Epoch 23/25) Training loss: 6.6242278638523295, Test loss: 10.340709686279297\n",
    "(Epoch 24/25) Training loss: 6.520543051754824, Test loss: 10.395054817199707\n",
    "```\n",
    "\n",
    "input contains image (human centered) height, isFacingForward\n",
    "\n",
    "output contains (x,y) coordinates for 22 body parts\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51a642c-7d3c-462e-972b-ebb8c177ad69",
   "metadata": {},
   "source": [
    "## 1. Boundaries detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b622b-4ff1-4538-ac66-c2e8832f41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\bihta\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 0/25) Training loss: 9.297963367219557, Test loss: 1.3056598901748657\n",
      "(Epoch 1/25) Training loss: 5.771352807680766, Test loss: 1.1232725381851196\n",
      "(Epoch 2/25) Training loss: 4.444771765878326, Test loss: 1.0578163862228394\n"
     ]
    }
   ],
   "source": [
    "from torch import optim, nn\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(bmodel.parameters(), lr=0.0007, momentum=0.7)\n",
    "criterion = nn.L1Loss(reduction='sum') \n",
    "device = 'cuda'\n",
    "bmodel.to(device)\n",
    "train_losses, test_losses = [], []\n",
    "epochs = 25\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, details, targets, confidences, boundaries in trainloader:\n",
    "        images, boundaries = images.to(device), boundaries.to(device)\n",
    "        #1. forward pass\n",
    "        ps = bmodel.forward(images)\n",
    "        #2. calculate loss\n",
    "        loss = criterion(ps, boundaries)\n",
    "        #0. Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #3. run backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Take an update step and few the new weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "#         print(loss.item())\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            bmodel.eval()\n",
    "            for images, details, targets, confidences, boundaries in testloader:\n",
    "                images, boundaries = images.to(device), boundaries.to(device)\n",
    "                ps = bmodel.forward(images)\n",
    "                test_loss += my_loss(ps, boundaries)\n",
    "\n",
    "        \n",
    "        bmodel.train()\n",
    "        \n",
    "        test_loss = test_loss/len(testloader)\n",
    "        train_loss = running_loss/len(trainloader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"(Epoch {e}/{epochs}) Training loss: {train_loss}, Test loss: {test_loss}\")\n",
    "        \n",
    "PATH = 'bmodel.m'\n",
    "torch.save(bmodel.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19559b1f-16ca-4d6e-95d2-bd38a7745985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0007, momentum=0.7)\n",
    "# criterion = nn.MSELoss() \n",
    "criterion = nn.L1Loss(reduction='sum') \n",
    "bceloss = nn.BCELoss()\n",
    "bceloss_w = 4\n",
    "print(torch.cuda.get_device_name(0))\n",
    "train_losses, test_losses = [], []\n",
    "epochs = 25\n",
    "model.to(device)\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, details, targets, confidences, boundaries in trainloader:\n",
    "        images, details, targets, confidences = images.to(device), details.to(device), targets.to(device), confidences.to(device), boundaries.to(device)\n",
    "#         print(images.shape, targets.shape)\n",
    "        #1. forward pass\n",
    "        ps = model.forward(images, details)\n",
    "#         print(ps2, confidences)\n",
    "#         print(np.min(ps.cpu().detach().numpy()), np.min(targets.cpu().detach().numpy()))\n",
    "#         print(np.max(ps.cpu().detach().numpy()), np.max(targets.cpu().detach().numpy()))\n",
    "#         print(np.min(details.cpu().detach().numpy()))\n",
    "#         print(np.max(details.cpu().detach().numpy()))\n",
    "        #2. calculate loss\n",
    "        loss = criterion(ps, targets)\n",
    "#         _loss = criterion(ps, targets)\n",
    "#         _bceloss = bceloss(ps2, confidences) / bceloss_w\n",
    "#         print(_bceloss.item())\n",
    "#         loss = _loss + _bceloss\n",
    "        #0. Clear the gradients, do this because gradients are accumulated\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #3. run backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Take an update step and few the new weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "#         print(loss.item())\n",
    "    else:\n",
    "        test_loss = 0\n",
    "        accuracy = 0\n",
    "        \n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, details, targets, confidences, boundaries in testloader:\n",
    "                images, details, targets, confidences, boundaries = images.to(device), details.to(device), targets.to(device), confidences.to(device), boundaries.to(device)\n",
    "                ps = model.forward(images, details)\n",
    "#                 _loss = criterion(ps, targets)\n",
    "#                 _bceloss = bceloss(ps2, confidences) / bceloss_w\n",
    "#                 test_loss += _loss + _bceloss\n",
    "                test_loss += my_loss(ps, targets)\n",
    "\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        test_loss = test_loss/len(testloader)\n",
    "        train_loss = running_loss/len(trainloader)\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"(Epoch {e}/{epochs}) Training loss: {train_loss}, Test loss: {test_loss}\")\n",
    "        \n",
    "PATH = 'model.m'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f544c4-fe3e-4c32-af73-3fd12b7f67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "test_losses = [test_loss.to('cpu') for test_loss in test_losses]\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac03711-c019-4a33-9cd3-b782dc4ee56e",
   "metadata": {},
   "source": [
    "## Use our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f5312-c4ad-4b8f-ba9c-5726879c3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# test_transform = transforms.Compose([transforms.Resize((img_width,img_width)),\n",
    "#                                 transforms.ToTensor()])\n",
    "import helper\n",
    "# test_dataset = datasets.ImageFolder('train/', transform=test_transform)\n",
    "# test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=2, shuffle=True)\n",
    "images, details, labels, confidences = next(iter(testloader))\n",
    "\n",
    "\n",
    "model.to('cpu')\n",
    "\n",
    "# img = images[0].view(1, 195075)\n",
    "# img = torch.zeros(1,120000) + 222\n",
    "\n",
    "positions = []\n",
    "positions_expected = []\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model.forward(images, details)\n",
    "    logps_denormalized = logps \n",
    "    labels_denormalized = labels \n",
    "    \n",
    "    for body_index in range(22):\n",
    "        xyz = []\n",
    "        xyz_e = []\n",
    "        for pos_index in range(2):\n",
    "            xyz.append(logps_denormalized[0][body_index*2+pos_index])\n",
    "            xyz_e.append(labels_denormalized[0][body_index*2+pos_index])\n",
    "        positions.append(xyz)\n",
    "        positions_expected.append(xyz_e)\n",
    "#     print(list(model.parameters()))\n",
    "\n",
    "positions = np.array(positions)\n",
    "positions_expected = np.array(positions_expected)\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "# zdata = positions.T[1]\n",
    "xdata = positions.T[0]\n",
    "ydata = positions.T[1]\n",
    "ax.set_xlim((0,1))\n",
    "ax.set_ylim((0,1))\n",
    "ax.scatter(xdata, ydata);\n",
    "\n",
    "# zdata_e = positions_expected.T[1]\n",
    "xdata_e = positions_expected.T[0]\n",
    "ydata_e = positions_expected.T[1]\n",
    "ax.scatter(xdata_e, ydata_e, marker='^')\n",
    "helper.imshow(images[0], xdata=xdata, ydata=ydata)\n",
    "\n",
    "for i, pos in enumerate(positions):\n",
    "    label = None\n",
    "    if i == 4:\n",
    "        label = \"Neck\"\n",
    "    elif i == 5:\n",
    "        label = \"Head\"\n",
    "    elif i == 9:\n",
    "        label = \"LeftHand\"\n",
    "    elif i == 12:\n",
    "        label = \"LeftFoot\"\n",
    "    elif i == 17:\n",
    "        label = \"RightHand\"\n",
    "    elif i == 20:\n",
    "        label = \"RightFoot\"\n",
    "    if label is not None:\n",
    "        label = label + \"{:.2f}\".format(confidences[0][i].item())\n",
    "        ax.text(pos[0], pos[1], label, None, color=\"green\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac4ee5-6e12-4f35-9657-d41bef0f259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for i, pos in enumerate(positions):\n",
    "    pos_exp = positions_expected[i]\n",
    "    \n",
    "    dist = np.linalg.norm(pos_exp-pos)\n",
    "    distances.append(dist)\n",
    "\n",
    "print(\"Total distance\", np.array(distances).sum())\n",
    "plt.plot(distances, label='Distances')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d73189-8436-4a42-a919-35dc8c9abbb1",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839fce70-e1e3-48d4-b581-e534509e3ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "motions_test2 = MotionDataset('test2/input.csv', 'test2', test_transforms, minmax, minmax_z)\n",
    "testloader2 = torch.utils.data.DataLoader(motions_test2, batch_size=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5563366e-e345-44f4-81f9-1062a6bc65a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffadcf0-d063-41c4-a7d8-8ba801d9ca3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
